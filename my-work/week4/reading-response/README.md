### Podcast Response - Week 4
To [Virginia Eubanks, AUTOMATING INEQUALITY](https://www.writersvoice.net/tag/virginia-eubanks/) with Virginia Eubanks and Francesca Rheannon (2019).

* ##### *How do technical tools promise to "fair out" the remaining discrimination that exist in social/welfare systems? In how far can they succeed, in which ways do they fail?*
    In the US, technical tools are used to serve as a filter of the alleged "truly worthy" people for the welfare against "discriminatory decision-making". The previous approach of in-person contact could potentially involve biased personal judgement regarding races, sex, and other factors considered as a deviant social status. Therefore, the technical approach claims to "fair out" such bias by viewing people with the same objective lens in order to decide who are worth the help. It has perhaps succeeded in directing the resources to the most desparate people according to the pre-built metric, however, definitely failed as an inhuman way of dealing with people. Tragic stories as is told by Eubanks happened, in which people in need for help are deprived of their own rights due to the less negotiable nature of the technical system. Also, such a seemingly object yet inhuman factor disovles the accountability of faulty decisions, protecting the system designers from being accused as the real decision maker. 


* ##### *Imagine, what could this (following quotes) mean in the widest sense?*
    > "The state doesn't need a cop to kill a person" and "electronic incarceration"

    By "modernizing" the social/welfare system, the state power can easily deprive or limit people's access to essential resources, such as housing and food, sometimes without noticing it. To my understanding, people are constantly imprisoned in an "electronic incarceration", which include and not limited to being monitored by digital surveillance which other policies would act upon.


* ##### *What do you understand this to mean?*
    > "systems act as a kind of 'empathy-overwrite'"

    The systems try to convince us that the deprived group of people are not worth our empathy by proving them somehow inadequate or deviant according to the whatever the standard is. It is an irrational rationalization of promoting inequality in public services against our natural empathy to the poor.


* ##### *China is much more advanced and expansive when it comes to applying technical solutions to societal processes or instant challenges ([recent example](https://www.nytimes.com/2020/03/01/business/china-coronavirus-surveillance.html?)). Try to point example cases in China that are in accordance or in opposition to the problematics discussed in the podcast. Perhaps you can think of*
    > "technical systems not well thought-through about what their impace on human beings is"

    Facial recognition as payment method at retail stores and vending machines has been one of the most impactful but controversial technical adoption in China. Unlike the approach as on a personal mobile phone which presumeably store the more sophisticated facial features at a digital black box inside the device, most of the facial recognitions "in the wild" require online access to our biometrics, most probably a simplied one represented by 2D RGB images or sequence. Therefore, it could be significantly more vulnerable than what we already have just for the sake that one bit more of convinience. However, for technology companies, it seems absolutely appealing due to more user data collection and profitability. Besides, there have been many reports of backdoors in Chinese softwares with government association. With the technically empowered govenmental patriachy, people could hardly have an alternative to unlimitedly ceding their rights, branded as an inevitable trend of technological advancement.



    